{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71ce33a8",
   "metadata": {},
   "source": [
    "### Q1: What are missing values in a dataset? Why is it essential to handle missing values? Name some algorithms that are not affected by missing values.\n",
    "\n",
    "Missing values refer to the absence of a particular value or attribute for a given observation in a dataset. In other words, it is a situation where some data is not available or cannot be recorded for a particular observation.\n",
    "\n",
    "Handling missing values is essential because they can have a significant impact on the analysis and modeling of the data. If missing values are not handled properly, they can result in biased or incorrect analysis and modeling, leading to unreliable results. Missing data can also reduce the accuracy of the model and reduce the predictive power of the algorithm.\n",
    "\n",
    "Some of the algorithms that are not affected by missing values are:\n",
    "\n",
    "1. Decision trees: Decision trees can handle missing values by replacing them with a default value or predicting the missing value using available data.\n",
    "\n",
    "2. Random Forests: Random forests are also robust to missing data as they can work with missing values without imputing or replacing them.\n",
    "\n",
    "3. Gradient Boosting: Gradient boosting algorithms can also handle missing values by ignoring them or predicting the missing value using available data.\n",
    "\n",
    "4. Naive Bayes: Naive Bayes classifiers are also insensitive to missing values as they work with probabilities and are not affected by the absence of a particular attribute.\n",
    "\n",
    "5. K-Nearest Neighbors (KNN): KNN is also not affected by missing data as it ignores the missing values and considers only the available data for making predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d4d35b",
   "metadata": {},
   "source": [
    "### Q2: List down techniques used to handle missing data. Give an example of each with python code.\n",
    "\n",
    "1. Deletion: In this technique, we remove the rows or columns with missing data from the dataset. There are two types of deletion techniques:\n",
    "\n",
    "    a. Listwise Deletion: In this technique, we remove the entire row if it contains any missing value.\n",
    "\n",
    "    b. Pairwise Deletion: In this technique, we remove the missing values on a per-variable basis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0f8e375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     A    B   C\n",
      "0  1.0  5.0   9\n",
      "3  4.0  8.0  12\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Create a sample dataframe with missing values\n",
    "df = pd.DataFrame({'A': [1, 2, np.nan, 4], 'B': [5, np.nan, 7, 8], 'C': [9, 10, 11, 12]})\n",
    "\n",
    "# Remove rows with missing values\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac73845",
   "metadata": {},
   "source": [
    "2. Imputation: In this technique, we fill in the missing values with estimated values based on the available data. There are different types of imputation techniques:\n",
    "\n",
    "    a. Mean/Median/Mode Imputation: In this technique, we replace the missing value with the mean/median/mode value of the available data.\n",
    "\n",
    "    b. KNN Imputation: In this technique, we use the K-nearest neighbor algorithm to impute the missing values.\n",
    "\n",
    "    c. Regression Imputation: In this technique, we use regression analysis to estimate the missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b700d242",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          A         B     C\n",
      "0  1.000000  5.000000   9.0\n",
      "1  2.000000  6.666667  10.0\n",
      "2  2.333333  7.000000  11.0\n",
      "3  4.000000  8.000000  12.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Create a sample dataframe with missing values\n",
    "df = pd.DataFrame({'A': [1, 2, np.nan, 4], 'B': [5, np.nan, 7, 8], 'C': [9, 10, 11, 12]})\n",
    "\n",
    "# Create an imputer object with mean strategy\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "\n",
    "# Impute the missing values\n",
    "df_imputed = imputer.fit_transform(df)\n",
    "\n",
    "# Convert the numpy array back to dataframe\n",
    "df_imputed = pd.DataFrame(df_imputed, columns=df.columns)\n",
    "\n",
    "print(df_imputed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037b41e6",
   "metadata": {},
   "source": [
    "3. Prediction: In this technique, we use machine learning algorithms to predict the missing values based on the available data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0634990",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     A    B     C\n",
      "0  1.0  5.0   9.0\n",
      "1  2.0  6.0  10.0\n",
      "2  3.0  7.0  11.0\n",
      "3  4.0  8.0  12.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "# Create a sample dataframe with missing values\n",
    "df = pd.DataFrame({'A': [1, 2, np.nan, 4], 'B': [5, np.nan, 7, 8], 'C': [9, 10, 11, 12]})\n",
    "\n",
    "# Create an imputer object with KNN strategy\n",
    "imputer = KNNImputer(n_neighbors=2)\n",
    "\n",
    "# Impute the missing values\n",
    "df_imputed = imputer.fit_transform(df)\n",
    "\n",
    "# Convert the numpy array back to dataframe\n",
    "df_imputed = pd.DataFrame(df_imputed, columns=df.columns)\n",
    "\n",
    "print(df_imputed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973d2af0",
   "metadata": {},
   "source": [
    "### Q3: Explain the imbalanced data. What will happen if imbalanced data is not handled?\n",
    "\n",
    "Imbalanced data refers to a situation in which the number of observations in one class of a binary classification problem is significantly higher than the number of observations in the other class. For example, consider a binary classification problem in which we want to predict whether a bank loan applicant will default or not. If we have 95% non-defaulters and only 5% defaulters in the dataset, then the data is imbalanced.\n",
    "\n",
    "If imbalanced data is not handled, it can result in biased or inaccurate predictions, especially for the minority class. In other words, the model trained on imbalanced data will have a high accuracy rate for the majority class and a low accuracy rate for the minority class. This is because most machine learning algorithms tend to learn from the majority class, leading to poor performance on the minority class.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0dcb57a",
   "metadata": {},
   "source": [
    "### Q4: What are Up-sampling and Down-sampling? Explain with an example when up-sampling and down- sampling are required.\n",
    "\n",
    "\n",
    "\n",
    "`Up-sampling` involves increasing the number of observations in the minority class to balance the dataset. This can be done by replicating existing observations or generating synthetic observations using techniques like SMOTE. The goal is to increase the representation of the minority class in the dataset.\n",
    "\n",
    "For example, suppose we have a binary classification problem in which we want to predict whether a customer will churn or not. If we have only 10% of customers who churned, we can up-sample the minority class by replicating the existing observations or generating synthetic observations. This will increase the representation of the minority class in the dataset and improve the performance of the model.\n",
    "\n",
    "\n",
    "\n",
    "`Down-sampling` involves decreasing the number of observations in the majority class to balance the dataset. This can be done by randomly removing observations from the majority class. The goal is to reduce the representation of the majority class in the dataset.\n",
    "\n",
    "For example, suppose we have a binary classification problem in which we want to predict whether a customer will buy a product or not. If we have 90% of customers who bought the product, we can down-sample the majority class by randomly removing observations. This will reduce the representation of the majority class in the dataset and improve the performance of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5b7e91",
   "metadata": {},
   "source": [
    "# Q5: What is data Augmentation? Explain SMOTE.\n",
    "\n",
    "Data augmentation is a technique used in machine learning to increase the size of a dataset by creating new examples from the existing ones. The idea is to use various transformations and manipulations of the original data to generate new data points that are similar but not identical to the original ones. This technique is widely used in computer vision, natural language processing, and speech recognition applications.\n",
    "\n",
    "SMOTE (Synthetic Minority Over-sampling Technique) is a data augmentation technique used to handle imbalanced data in machine learning. It works by generating synthetic examples for the minority class by interpolating between the existing minority class examples. Here's how SMOTE works:\n",
    "\n",
    "1. For each minority class example, SMOTE selects k nearest neighbors in the minority class.\n",
    "\n",
    "2. SMOTE generates synthetic examples by randomly selecting one of the k nearest neighbors and creating a new example by interpolating between the selected neighbor and the original example. The interpolation is done by selecting a random point along the line connecting the two examples.\n",
    "\n",
    "3. SMOTE repeats the process for each minority class example until the desired balance between the majority and minority classes is achieved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce7ad6e",
   "metadata": {},
   "source": [
    "### Q6: What are outliers in a dataset? Why is it essential to handle outliers?\n",
    "\n",
    "Outliers in a dataset are observations that are significantly different from other observations in the dataset. In other words, outliers are data points that are located far away from the rest of the data points in a distribution. Outliers can be caused by errors in data collection, measurement errors, or natural variations in the data.\n",
    "\n",
    "Handling outliers is essential in data analysis and machine learning because they can have a significant impact on the results of the analysis. Here are some reasons why handling outliers is important:\n",
    "\n",
    "1. Outliers can skew the distribution of the data and make it difficult to accurately estimate statistics such as the mean, median, and standard deviation.\n",
    "\n",
    "2. Outliers can lead to biased or inaccurate models in machine learning. Many machine learning algorithms are sensitive to outliers and can be influenced by them, resulting in poor performance.\n",
    "\n",
    "3. Outliers can also affect the evaluation metrics used to assess the performance of machine learning models. Evaluation metrics such as accuracy, precision, recall, and F1 score can give misleading results when outliers are present in the data.\n",
    "\n",
    "4. Outliers can cause problems in data visualization, as they can distort the scale of the visualization and make it difficult to interpret the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ab9427",
   "metadata": {},
   "source": [
    "### Q7: You are working on a project that requires analyzing customer data. However, you notice that some of the data is missing. What are some techniques you can use to handle the missing data in your analysis?\n",
    "\n",
    "Handling missing data is essential in data analysis as missing data can have a significant impact on the results of the analysis. Here are some techniques that can be used to handle missing data:\n",
    "\n",
    "1. Deleting missing data: This involves deleting all the rows or columns with missing values. This method can be effective if the missing data is small and randomly distributed across the dataset.\n",
    "\n",
    "2. Imputing missing data: This involves filling in missing values with estimated values. Some commonly used methods for imputing missing data include mean imputation, median imputation, mode imputation, and regression imputation.\n",
    "\n",
    "3. Using machine learning models: Machine learning models such as k-nearest neighbors (KNN) and decision trees can be used to impute missing values.\n",
    "\n",
    "4. Multiple imputation: This involves creating multiple imputations of the missing data and using them to estimate the parameters of interest. This method can provide more accurate estimates than single imputation methods.\n",
    "\n",
    "5. Domain-specific knowledge: In some cases, domain-specific knowledge can be used to impute missing data. For example, if the missing data is related to a customer's age, their birth year or estimated age can be used to impute the missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7459eb86",
   "metadata": {},
   "source": [
    "### Q9: Suppose you are working on a medical diagnosis project and find that the majority of patients in the dataset do not have the condition of interest, while a small percentage do. What are some strategies you can use to evaluate the performance of your machine learning model on this imbalanced dataset?\n",
    "\n",
    "When working with imbalanced datasets, the performance of machine learning models can be biased towards the majority class. Here are some strategies that can be used to evaluate the performance of machine learning models on imbalanced datasets:\n",
    "\n",
    "1. Confusion matrix: A confusion matrix can be used to evaluate the performance of a machine learning model on an imbalanced dataset. The confusion matrix provides a breakdown of the number of true positives, true negatives, false positives, and false negatives. From the confusion matrix, metrics such as accuracy, precision, recall, and F1-score can be calculated to evaluate the model's performance.\n",
    "\n",
    "2. Resampling techniques: Resampling techniques such as over-sampling the minority class or under-sampling the majority class can be used to balance the dataset. This can help improve the performance of the machine learning model by reducing the bias towards the majority class.\n",
    "\n",
    "3. Cost-sensitive learning: Cost-sensitive learning involves assigning different costs to different types of classification errors. This can be useful in imbalanced datasets where the cost of misclassifying the minority class is higher than the cost of misclassifying the majority class.\n",
    "\n",
    "4. Evaluation metrics: Instead of using traditional evaluation metrics such as accuracy, precision, and recall, evaluation metrics that are better suited for imbalanced datasets can be used. Examples of such metrics include the area under the receiver operating characteristic curve (AUC-ROC) and the area under the precision-recall curve (AUC-PR)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46792682",
   "metadata": {},
   "source": [
    "### Q10: When attempting to estimate customer satisfaction for a project, you discover that the dataset is unbalanced, with the bulk of customers reporting being satisfied. What methods can you employ to balance the dataset and down-sample the majority class?\n",
    "\n",
    "When dealing with an unbalanced dataset where the majority class is significantly larger than the minority class, down-sampling the majority class can help balance the dataset. Here are some methods that can be employed to down-sample the majority class:\n",
    "\n",
    "1. Random under-sampling: This method involves randomly selecting a subset of the majority class samples to match the size of the minority class.\n",
    "\n",
    "2. Tomek Links: This method involves removing samples that form a \"Tomek link\" between the majority and minority class, which are samples that are the nearest neighbors of each other.\n",
    "\n",
    "3. Cluster Centroids: This method involves generating centroids based on clustering algorithms and removing the majority class samples closest to the centroids."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be71c425",
   "metadata": {},
   "source": [
    "### Q11: You discover that the dataset is unbalanced with a low percentage of occurrences while working on a project that requires you to estimate the occurrence of a rare event. What methods can you employ to balance the dataset and up-sample the minority class?\n",
    "\n",
    "When dealing with an unbalanced dataset where the minority class is significantly smaller than the majority class, up-sampling the minority class can help balance the dataset. Here are some methods that can be employed to up-sample the minority class:\n",
    "\n",
    "1. Random over-sampling: This method involves randomly duplicating samples from the minority class to match the size of the majority class.\n",
    "\n",
    "2. SMOTE (Synthetic Minority Over-sampling Technique): This method involves generating synthetic samples based on the existing minority class samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a9a08e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
