{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9391d942",
   "metadata": {},
   "source": [
    "### Q1. What is the relationship between polynomial functions and kernel functions in machine learning algorithms?\n",
    "\n",
    "olynomial functions and kernel functions are both used in machine learning algorithms for feature mapping and transforming data into a higher dimensional space.\n",
    "\n",
    "Polynomial functions are a type of function that involves powers and coefficients of a variable, such as x, x^2, x^3, etc. In machine learning, polynomial functions are often used as feature maps to transform data into a higher dimensional space, where linear models can be more effective. This is known as polynomial regression, which aims to find the best-fit polynomial curve to a set of data points.\n",
    "\n",
    "On the other hand, kernel functions are a more general type of function that can be used for feature mapping. Kernel functions are typically used in kernel methods, such as support vector machines (SVMs), which are used for classification and regression tasks. The kernel function computes the dot product between two transformed feature vectors, without explicitly computing the transformation itself. This allows the algorithm to operate in a higher dimensional space without the computational cost of actually transforming the data.\n",
    "\n",
    "Interestingly, some kernel functions, such as the polynomial kernel function, are based on polynomial functions. The polynomial kernel function computes the dot product between two polynomial feature vectors, effectively transforming the data into a higher dimensional space. However, the kernel trick allows this to be done without explicitly computing the polynomial transformation, making it much more efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0905731",
   "metadata": {},
   "source": [
    "### Q2. How can we implement an SVM with a polynomial kernel in Python using Scikit-learn?\n",
    "\n",
    "To implement an SVM with a polynomial kernel in Python using Scikit-learn, we can follow these steps:\n",
    "\n",
    "1. Import the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2870aca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3839696",
   "metadata": {},
   "source": [
    "2. Generate some random data using the make_classification function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9fc3fd03",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_classification(n_samples=1000, n_features=4, n_informative=2, n_redundant=0, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516afd26",
   "metadata": {},
   "source": [
    "3. Split the data into training and testing sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5fe1ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d9b4dc",
   "metadata": {},
   "source": [
    "4. Create an SVM object with the polynomial kernel and fit the training data:`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96c5bba3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(kernel='poly')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = svm.SVC(kernel='poly', degree=3)\n",
    "clf.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079b7d5d",
   "metadata": {},
   "source": [
    "5. Predict the class labels for the testing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2a6afb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2002486d",
   "metadata": {},
   "source": [
    "6. Calculate the accuracy score of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6759db03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8766666666666667\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1b551d",
   "metadata": {},
   "source": [
    "### Q3. How does increasing the value of epsilon affect the number of support vectors in SVR?\n",
    "\n",
    "In Support Vector Regression (SVR), the value of epsilon determines the width of the epsilon-insensitive zone around the regression line, within which no errors are penalized. Increasing the value of epsilon results in a wider epsilon-insensitive zone, which allows more training points to be within the margin and therefore not treated as support vectors. This means that the number of support vectors decreases as the value of epsilon is increased.\n",
    "\n",
    "When epsilon is small, the regression line must fit the training points more closely, resulting in a narrower margin and a larger number of support vectors. Conversely, when epsilon is large, the regression line can afford to be more forgiving of training points that fall within the epsilon-insensitive zone, resulting in a wider margin and fewer support vectors.\n",
    "\n",
    "It's important to note that increasing the value of epsilon also affects the generalization performance of the SVR model. A smaller value of epsilon can lead to better generalization performance, but may also lead to overfitting on the training data. A larger value of epsilon can result in better generalization performance, but may also lead to underfitting.\n",
    "\n",
    "Therefore, the choice of epsilon should be made based on the balance between the model's generalization performance and the number of support vectors required for the model. Cross-validation can be used to find an optimal value of epsilon that maximizes the model's performance while keeping the number of support vectors reasonable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ffef6b4",
   "metadata": {},
   "source": [
    "### Q4. How does the choice of kernel function, C parameter, epsilon parameter, and gamma parameter affect the performance of Support Vector Regression (SVR)? Can you explain how each parameter works and provide examples of when you might want to increase or decrease its value?\n",
    "\n",
    "\n",
    "The choice of kernel function, C parameter, epsilon parameter, and gamma parameter can significantly affect the performance of Support Vector Regression (SVR). Let's take a look at each parameter in turn and how it affects the SVR model:\n",
    "\n",
    "1. Kernel function: The kernel function is used to transform the input data into a higher-dimensional feature space, where it is easier to separate the classes or predict the continuous target variable. The choice of kernel function depends on the nature of the data and the problem you are trying to solve. For example, if the data is linearly separable, the linear kernel function may be the best choice. On the other hand, if the data has a more complex structure, the polynomial, radial basis function (RBF), or sigmoid kernel functions may be more appropriate. The polynomial kernel function can capture non-linear relationships between the input features, while the RBF kernel function is good at modeling non-linear and complex patterns in the data. The sigmoid kernel function is useful for binary classification problems.\n",
    "\n",
    "2. C parameter: The C parameter controls the trade-off between maximizing the margin and minimizing the training error. It determines the penalty for misclassifying or mispredicting the training samples. A higher value of C leads to a narrower margin and more misclassification/misprediction errors. In other words, a high C value will result in a more complex model that tries to fit the training data as closely as possible, potentially leading to overfitting. A lower value of C will result in a wider margin and fewer errors, but may result in underfitting. Therefore, you should increase C if you suspect that the model is underfitting and decrease C if you suspect that the model is overfitting.\n",
    "\n",
    "3. Epsilon parameter: The epsilon parameter determines the width of the epsilon-insensitive tube around the regression line, within which no errors are penalized. A higher value of epsilon allows more training points to fall within the margin and not be treated as support vectors, potentially leading to a simpler model. However, a higher value of epsilon may also result in poorer generalization performance. On the other hand, a lower value of epsilon leads to a narrower epsilon-insensitive zone and more support vectors, resulting in a more complex model. Therefore, you should increase epsilon if you want to simplify the model and decrease epsilon if you want to increase the model complexity.\n",
    "\n",
    "4. Gamma parameter: The gamma parameter determines the shape of the RBF kernel and controls the smoothness of the decision boundary. A high value of gamma results in a more complex decision boundary that is sensitive to small variations in the input data. A low value of gamma results in a smoother decision boundary that is less sensitive to noise and variations in the input data. Therefore, you should increase gamma if you want to fit the model more closely to the training data and decrease gamma if you want to increase the model's generalization performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dafd7daf",
   "metadata": {},
   "source": [
    "### Q5. Assignment:\n",
    "* Import the necessary libraries and load the dataset\n",
    "*  Split the dataset into training and testing sets\n",
    "* Preprocess the data using any technique of your choice (e.g. scaling, normalization\n",
    "* Create an instance of the SVC classifier and train it on the training data\n",
    "* Use the trained classifier to predict the labels of the testing data\n",
    "* Evaluate the performance of the classifier using any metric of your choice (e.g. accuracy, precision, recall, F1-score)\n",
    "* Tune the hyperparameters of the SVC classifier using GridSearchCV or RandomizedSearchCV to improve its performance\n",
    "* Train the tuned classifier on the entire dataset\n",
    "* Save the trained classifier to a file for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4875578",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c918c15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_diabetes(return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c6198bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y,\n",
    "                                                    test_size=0.3,\n",
    "                                                    random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3cf127f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e9d1719",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC()"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc = SVC()\n",
    "svc.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "39bbe09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = svc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "46bb2e5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy :  0.022556390977443608\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy : \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5662e14a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_split.py:676: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'C': 0.1, 'gamma': 0.1, 'kernel': 'linear'}\n"
     ]
    }
   ],
   "source": [
    "param_grid = {'C': [0.1, 1, 10],\n",
    "              'kernel': ['linear', 'rbf', 'poly'], \n",
    "              'gamma': [0.1, 1, 10]}\n",
    "\n",
    "grid_search = GridSearchCV(svc, param_grid=param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(\"Best parameters:\", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7f8e294e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=0.1, gamma=0.1, kernel='linear')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc_tuned = SVC(C=0.1, kernel='linear', gamma=0.1)\n",
    "svc_tuned.fit(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e949c94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('svc_tuned.pkl', 'wb') as file:\n",
    "    pickle.dump(svc_tuned, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53035bfa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
