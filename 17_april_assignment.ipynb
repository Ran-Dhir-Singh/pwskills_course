{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d593440f",
   "metadata": {},
   "source": [
    "### Q1. What is Gradient Boosting Regression?\n",
    "\n",
    "Gradient Boosting Regression is a machine learning algorithm that belongs to the family of boosting algorithms. It is used for solving regression problems, i.e., problems where the goal is to predict a continuous numerical value, such as the price of a house or the temperature of a city.\n",
    "\n",
    "The Gradient Boosting Regression algorithm works by combining multiple weak regression models to create a stronger model. The algorithm starts by fitting an initial model to the data, which can be a simple model such as a decision tree with a small depth. The algorithm then iteratively adds new weak models to the ensemble, where each new model is trained to correct the errors of the previous model.\n",
    "\n",
    "In each iteration, the algorithm calculates the gradient of the loss function with respect to the predicted values of the previous model. The gradient is then used to fit a new regression model to the residuals, i.e., the difference between the predicted values of the previous model and the true values. The new model is then added to the ensemble with a weight that is determined by a learning rate hyperparameter.\n",
    "\n",
    "The final prediction of the Gradient Boosting Regression algorithm is a weighted sum of the predictions of all weak models in the ensemble. The weights are determined by the performance of each weak model on the training data.\n",
    "\n",
    "The key benefits of Gradient Boosting Regression are its ability to handle non-linear and complex relationships between the features and the target variable, and its robustness to noise and outliers in the data. However, the algorithm can be sensitive to overfitting and may require careful tuning of the hyperparameters, such as the learning rate, the number of weak models, and their complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1a1a18",
   "metadata": {},
   "source": [
    "### Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a simple regression problem as an example and train the model on a small dataset. Evaluate the model's performance using metrics such as mean squared error and R-squared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "99d96926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 7191.08013677331\n",
      "R-Squared Score: 0.5590719366306647\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "class GradientBoostingRegressor(BaseEstimator,RegressorMixin):\n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.trees = []\n",
    "        self.intercept = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.intercept = np.mean(y)\n",
    "        f = np.full_like(y, self.intercept)\n",
    "        for i in range(self.n_estimators):\n",
    "            r = y - f\n",
    "            tree = DecisionTreeRegressor(max_depth=self.max_depth)\n",
    "            tree.fit(X, r)\n",
    "            self.trees.append(tree)\n",
    "            f += self.learning_rate * tree.predict(X)\n",
    "\n",
    "    def predict(self, X):\n",
    "        f = np.full(X.shape[0], self.intercept)\n",
    "        for tree in self.trees:\n",
    "            f += self.learning_rate * tree.predict(X)\n",
    "        return f\n",
    "\n",
    "# Generate a simple regression dataset\n",
    "X, y = make_regression(n_samples=100, n_features=10, noise=0.2, random_state=42)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "train_X, train_y = X[:80], y[:80]\n",
    "test_X, test_y = X[80:], y[80:]\n",
    "\n",
    "# Train the gradient boosting model\n",
    "model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3)\n",
    "model.fit(train_X, train_y)\n",
    "\n",
    "# Evaluate the model's performance on the testing set\n",
    "y_pred = model.predict(test_X)\n",
    "mse = mean_squared_error(test_y, y_pred)\n",
    "r2 = r2_score(test_y, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"R-Squared Score:\", r2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7bef48",
   "metadata": {},
   "source": [
    "### Q3. Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to optimise the performance of the model. Use grid search or random search to find the best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ebd35eca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 150}\n",
      "Best Mean Test Score: 9944.879791365898\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Define a parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'learning_rate': [0.01, 0.1, 0.5],\n",
    "    'max_depth': [3, 4, 5]\n",
    "}\n",
    "\n",
    "# Create a grid search object with MSE as the scoring metric\n",
    "grid_search = GridSearchCV(GradientBoostingRegressor(), param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "\n",
    "# Fit the grid search object to the training data\n",
    "grid_search.fit(train_X, train_y)\n",
    "\n",
    "# Print the best hyperparameters and corresponding mean test score\n",
    "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
    "print(\"Best Mean Test Score:\", -grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b9d0b2",
   "metadata": {},
   "source": [
    "### Q4. What is a weak learner in Gradient Boosting?\n",
    "\n",
    "In Gradient Boosting, a weak learner is a simple and relatively weak predictive model that is used as the base model in an ensemble of models. Specifically, a weak learner is a model that is only slightly better than random guessing on the training data. In Gradient Boosting, the weak learner is typically a decision tree with a very small depth or number of leaf nodes.\n",
    "\n",
    "The idea behind Gradient Boosting is to iteratively improve the predictions of the weak learner by fitting each successive model to the negative gradient of the loss function with respect to the predicted values of the previous model. By doing this, the subsequent models focus on the areas where the previous models performed poorly, and thus the ensemble model gradually improves its overall predictive accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aad2671",
   "metadata": {},
   "source": [
    "### Q5. What is the intuition behind the Gradient Boosting algorithm?\n",
    "\n",
    "The Gradient Boosting algorithm is a machine learning technique for building powerful predictive models. The intuition behind the algorithm is to combine several weak learners (often decision trees) into a strong learner that makes accurate predictions.\n",
    "\n",
    "The algorithm works by fitting the weak learners sequentially to the residuals of the previous weak learner. In other words, it first fits a weak learner to the data and then calculates the difference between the predicted values and the true values. The next weak learner is then fit to these residuals, and the process continues until a specified stopping criterion is reached (such as a maximum number of trees or a minimum improvement in performance).\n",
    "\n",
    "The key idea behind this approach is that the subsequent weak learners are trained to correct the errors made by the previous weak learners, leading to a strong ensemble model that is better than any of the individual weak learners.\n",
    "\n",
    "The name \"Gradient Boosting\" comes from the fact that the algorithm uses gradient descent optimization to minimize the loss function of the model. In each iteration, the algorithm calculates the negative gradient of the loss function with respect to the predicted values, and fits a weak learner to these gradients. This approach allows the model to learn from its mistakes and improve over time, leading to a highly accurate predictive model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba64c46",
   "metadata": {},
   "source": [
    "### Q6. How does Gradient Boosting algorithm build an ensemble of weak learners?\n",
    "\n",
    "The Gradient Boosting algorithm builds an ensemble of weak learners by iteratively adding new weak learners to the model. At each iteration, the algorithm fits a new weak learner to the residual errors of the current model, instead of fitting the weak learner to the original target variable. This means that each new weak learner focuses on the errors made by the current model, and tries to correct these errors.\n",
    "\n",
    "The new weak learner is then added to the current model by multiplying its predictions by a learning rate, and adding the resulting scaled predictions to the predictions of the current model. The learning rate controls the contribution of each new weak learner to the final model, and is typically a small value between 0.01 and 0.1.\n",
    "\n",
    "The process is repeated until a predetermined number of weak learners has been added to the model, or until the training error stops improving. The final model is then the sum of all the individual weak learners, each scaled by its corresponding learning rate. The result is a powerful model that can capture complex non-linear relationships between the input features and the target variable.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7142de",
   "metadata": {},
   "source": [
    "### Q7. What are the steps involved in constructing the mathematical intuition of Gradient Boosting algorithm?\n",
    "\n",
    "The mathematical intuition of Gradient Boosting algorithm can be broken down into the following steps:\n",
    "\n",
    "1. Define a loss function: The first step in building a Gradient Boosting model is to define a loss function that measures the difference between the predicted values and the actual values. In regression problems, the commonly used loss function is mean squared error, while for classification problems, cross-entropy loss or exponential loss is often used.\n",
    "\n",
    "2. Fit the first model: The next step is to fit the first model, usually a simple one like a decision tree, to the training data. The predicted values from this model are used as the starting point for the Gradient Boosting algorithm.\n",
    "\n",
    "3. Compute the residuals: The difference between the predicted values and the actual values are computed and treated as the residuals. The objective of Gradient Boosting is to fit a model to these residuals that can correct the errors made by the first model.\n",
    "\n",
    "4. Fit subsequent models to the residuals: A new model is fit to the residuals, and the predicted values from this model are added to the predictions from the previous model. This process continues until a predefined stopping criteria are met.\n",
    "\n",
    "5. Tune the hyperparameters: The hyperparameters of the Gradient Boosting algorithm, such as the learning rate, the number of trees, and the depth of each tree, are tuned to achieve the best performance on the validation data.\n",
    "\n",
    "6. Make predictions: The final step is to make predictions using the ensemble of models. The predictions are made by summing up the predictions from all the models in the ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44e26ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
