{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f03a5997",
   "metadata": {},
   "source": [
    "### Q1. A company conducted a survey of its employees and found that 70% of the employees use the company's health insurance plan, while 40% of the employees who use the plan are smokers. What is the probability that an employee is a smoker given that he/she uses the health insurance plan?\n",
    "\n",
    "To solve this problem using naive Bayes, we need to apply Bayes' theorem and assume that the features (smoker status and insurance plan usage) are conditionally independent given the class (employee status as smoker or non-smoker).\n",
    "\n",
    "Let S denote the event that an employee is a smoker, and H denote the event that an employee uses the company's health insurance plan. Then, we want to calculate the probability of S given H, i.e., P(S|H).\n",
    "\n",
    "Using Bayes' theorem, we have:\n",
    "\n",
    "P(S|H) = P(H|S) * P(S) / P(H)\n",
    "\n",
    "where P(H|S) is the probability of an employee using the health insurance plan given that he/she is a smoker, P(S) is the prior probability of an employee being a smoker, and P(H) is the overall probability of an employee using the health insurance plan.\n",
    "\n",
    "From the given information, we know that P(H) = 0.7 (since 70% of employees use the insurance plan), and P(S) is not directly given. However, we can estimate it from the information that 40% of employees who use the plan are smokers, i.e.,\n",
    "\n",
    "P(S ∩ H) = P(H|S) * P(S) = 0.4 * 0.7\n",
    "\n",
    "Thus, we can calculate P(S) as:\n",
    "\n",
    "P(S) = P(S ∩ H) / P(H) = (0.4 * 0.7) / 0.7 = 0.4\n",
    "\n",
    "Now, we can substitute the values into Bayes' theorem and calculate P(S|H) as:\n",
    "\n",
    "P(S|H) = P(H|S) * P(S) / P(H) = (0.4 * 0.4) / 0.7 = 0.2286 (rounded to four decimal places)\n",
    "\n",
    "Therefore, the probability that an employee is a smoker given that he/she uses the health insurance plan is 0.2286, or about 22.86%.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb59c8be",
   "metadata": {},
   "source": [
    "### Q2. What is the difference between Bernoulli Naive Bayes and Multinomial Naive Bayes?\n",
    "\n",
    "Bernoulli Naive Bayes and Multinomial Naive Bayes are two variants of the Naive Bayes algorithm used for text classification and other machine learning tasks. The main difference between them lies in the assumption they make about the distribution of the features.\n",
    "\n",
    "Bernoulli Naive Bayes assumes that the features are binary (i.e., they take on values of 0 or 1) and follows a Bernoulli distribution. This means that it is suitable for classification problems where the presence or absence of a feature is important, but the frequency or count of the feature is not relevant. An example of such a problem is spam detection, where the presence or absence of certain words in an email can indicate whether it is spam or not.\n",
    "\n",
    "In contrast, Multinomial Naive Bayes assumes that the features are counts of occurrences (i.e., integer values) and follows a Multinomial distribution. This means that it is suitable for classification problems where the frequency or count of a feature is important, such as document classification, where the number of times a word appears in a document can help determine its category.\n",
    "\n",
    "Another key difference between Bernoulli and Multinomial Naive Bayes is in the way they handle missing features. In Bernoulli Naive Bayes, missing features are assumed to have a value of 0, while in Multinomial Naive Bayes, they are ignored during training and set to 0 during prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25efb95",
   "metadata": {},
   "source": [
    "### Q3. How does Bernoulli Naive Bayes handle missing values?\n",
    "\n",
    "In Bernoulli Naive Bayes, missing values are typically handled by assuming that their value is 0, meaning that the corresponding feature is not present. This is because Bernoulli Naive Bayes assumes that the features are binary, and a missing value can be interpreted as the absence of that feature.\n",
    "\n",
    "For example, let's say we are using Bernoulli Naive Bayes for spam detection, and we have a feature for the presence of the word \"viagra\" in an email. If a particular email does not contain the word \"viagra\", then the corresponding value in the feature vector would be 0, indicating that the feature is not present in that email. If the value for this feature is missing, then we would assume that the word \"viagra\" is not present in the email, and the corresponding value would be set to 0.\n",
    "\n",
    "However, it is worth noting that the treatment of missing values in Bernoulli Naive Bayes may depend on the specific implementation or library used, as different approaches may be used to handle missing data. For example, some implementations may allow for imputing missing values based on the distribution of the feature, while others may simply ignore instances with missing values. In any case, it is important to consider the handling of missing values when applying Bernoulli Naive Bayes to a particular dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151dbbaa",
   "metadata": {},
   "source": [
    "### Q4. Can Gaussian Naive Bayes be used for multi-class classification?\n",
    "\n",
    "Yes, Gaussian Naive Bayes can be used for multi-class classification. The algorithm can be extended to handle more than two classes by using the \"one-vs-all\" or \"one-vs-rest\" approach, which is a common technique for multi-class classification problems.\n",
    "\n",
    "In the \"one-vs-all\" approach, we train a separate binary Gaussian Naive Bayes classifier for each class, with the goal of distinguishing that class from all other classes combined. During prediction, we use all of the classifiers to make a prediction for each input, and the class with the highest probability is selected as the predicted class.\n",
    "\n",
    "For example, let's say we have a dataset with three classes: A, B, and C. To apply Gaussian Naive Bayes for multi-class classification using the \"one-vs-all\" approach, we would train three separate binary classifiers: one for A vs. (B + C), one for B vs. (A + C), and one for C vs. (A + B). During prediction, we would compute the probability of each class for a given input using all three classifiers, and select the class with the highest probability.'/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593c0875",
   "metadata": {},
   "source": [
    "### Q5. Assignment:\n",
    "\n",
    "#### Data preparation:\n",
    "Download the \"Spambase Data Set\" from the UCI Machine Learning Repository (https://archive.ics.uci.edu/ml/datasets/Spambase). This dataset contains email messages, where the goal is to predict whether a message\n",
    "is spam or not based on several input features.\n",
    "\n",
    "#### Implementation:\n",
    "Implement Bernoulli Naive Bayes, Multinomial Naive Bayes, and Gaussian Naive Bayes classifiers using the\n",
    "scikit-learn library in Python. Use 10-fold cross-validation to evaluate the performance of each classifier on the\n",
    "dataset. You should use the default hyperparameters for each classifier.\n",
    "\n",
    "#### Results:\n",
    "Report the following performance metrics for each classifier:\n",
    "Accuracy\n",
    "Precision\n",
    "Recall\n",
    "F1 score\n",
    "\n",
    "#### Discussion:\n",
    "Discuss the results you obtained. Which variant of Naive Bayes performed the best? Why do you think that is\n",
    "the case? Are there any limitations of Naive Bayes that you observed?\n",
    "\n",
    "#### Conclusion:\n",
    "Summarise your findings and provide some suggestions for future work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "75ca3430",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB, GaussianNB\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a0f548",
   "metadata": {},
   "source": [
    "#### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "03a7c117",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/spambase/spambase.names', 'r') as f:\n",
    "    attribute_names = []\n",
    "    for line in f:\n",
    "        if line.startswith('|') or line.startswith(' '):\n",
    "            continue\n",
    "        elif line.startswith('1') or line.startswith('0'):\n",
    "            attribute_names.append(line.split('|')[0].strip())\n",
    "        else:\n",
    "            attribute_names.append(line.split(':')[0].strip())\n",
    "    attributes = [attr for attr in attribute_names if attr != '']\n",
    "    attributes = attributes[1:] + [attributes[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8e19afb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['word_freq_make', 'word_freq_address', 'word_freq_all', 'word_freq_3d', 'word_freq_our', 'word_freq_over', 'word_freq_remove', 'word_freq_internet', 'word_freq_order', 'word_freq_mail', 'word_freq_receive', 'word_freq_will', 'word_freq_people', 'word_freq_report', 'word_freq_addresses', 'word_freq_free', 'word_freq_business', 'word_freq_email', 'word_freq_you', 'word_freq_credit', 'word_freq_your', 'word_freq_font', 'word_freq_000', 'word_freq_money', 'word_freq_hp', 'word_freq_hpl', 'word_freq_george', 'word_freq_650', 'word_freq_lab', 'word_freq_labs', 'word_freq_telnet', 'word_freq_857', 'word_freq_data', 'word_freq_415', 'word_freq_85', 'word_freq_technology', 'word_freq_1999', 'word_freq_parts', 'word_freq_pm', 'word_freq_direct', 'word_freq_cs', 'word_freq_meeting', 'word_freq_original', 'word_freq_project', 'word_freq_re', 'word_freq_edu', 'word_freq_table', 'word_freq_conference', 'char_freq_;', 'char_freq_(', 'char_freq_[', 'char_freq_!', 'char_freq_$', 'char_freq_#', 'capital_run_length_average', 'capital_run_length_longest', 'capital_run_length_total', '1, 0.']\n"
     ]
    }
   ],
   "source": [
    "print(attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "52354bd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_freq_make</th>\n",
       "      <th>word_freq_address</th>\n",
       "      <th>word_freq_all</th>\n",
       "      <th>word_freq_3d</th>\n",
       "      <th>word_freq_our</th>\n",
       "      <th>word_freq_over</th>\n",
       "      <th>word_freq_remove</th>\n",
       "      <th>word_freq_internet</th>\n",
       "      <th>word_freq_order</th>\n",
       "      <th>word_freq_mail</th>\n",
       "      <th>...</th>\n",
       "      <th>char_freq_;</th>\n",
       "      <th>char_freq_(</th>\n",
       "      <th>char_freq_[</th>\n",
       "      <th>char_freq_!</th>\n",
       "      <th>char_freq_$</th>\n",
       "      <th>char_freq_#</th>\n",
       "      <th>capital_run_length_average</th>\n",
       "      <th>capital_run_length_longest</th>\n",
       "      <th>capital_run_length_total</th>\n",
       "      <th>1, 0.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.756</td>\n",
       "      <td>61</td>\n",
       "      <td>278</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.21</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.94</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.048</td>\n",
       "      <td>5.114</td>\n",
       "      <td>101</td>\n",
       "      <td>1028</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.23</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.010</td>\n",
       "      <td>9.821</td>\n",
       "      <td>485</td>\n",
       "      <td>2259</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   word_freq_make  word_freq_address  word_freq_all  word_freq_3d  \\\n",
       "0            0.00               0.64           0.64           0.0   \n",
       "1            0.21               0.28           0.50           0.0   \n",
       "2            0.06               0.00           0.71           0.0   \n",
       "3            0.00               0.00           0.00           0.0   \n",
       "4            0.00               0.00           0.00           0.0   \n",
       "\n",
       "   word_freq_our  word_freq_over  word_freq_remove  word_freq_internet  \\\n",
       "0           0.32            0.00              0.00                0.00   \n",
       "1           0.14            0.28              0.21                0.07   \n",
       "2           1.23            0.19              0.19                0.12   \n",
       "3           0.63            0.00              0.31                0.63   \n",
       "4           0.63            0.00              0.31                0.63   \n",
       "\n",
       "   word_freq_order  word_freq_mail  ...  char_freq_;  char_freq_(  \\\n",
       "0             0.00            0.00  ...         0.00        0.000   \n",
       "1             0.00            0.94  ...         0.00        0.132   \n",
       "2             0.64            0.25  ...         0.01        0.143   \n",
       "3             0.31            0.63  ...         0.00        0.137   \n",
       "4             0.31            0.63  ...         0.00        0.135   \n",
       "\n",
       "   char_freq_[  char_freq_!  char_freq_$  char_freq_#  \\\n",
       "0          0.0        0.778        0.000        0.000   \n",
       "1          0.0        0.372        0.180        0.048   \n",
       "2          0.0        0.276        0.184        0.010   \n",
       "3          0.0        0.137        0.000        0.000   \n",
       "4          0.0        0.135        0.000        0.000   \n",
       "\n",
       "   capital_run_length_average  capital_run_length_longest  \\\n",
       "0                       3.756                          61   \n",
       "1                       5.114                         101   \n",
       "2                       9.821                         485   \n",
       "3                       3.537                          40   \n",
       "4                       3.537                          40   \n",
       "\n",
       "   capital_run_length_total  1, 0.  \n",
       "0                       278      1  \n",
       "1                      1028      1  \n",
       "2                      2259      1  \n",
       "3                       191      1  \n",
       "4                       191      1  \n",
       "\n",
       "[5 rows x 58 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_df = pd.read_csv(\"data/spambase/spambase.data\",names=attributes)\n",
    "spam_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7e8abebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_df = spam_df.rename(columns={'1, 0.' : 'target'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "dc416570",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_freq_make</th>\n",
       "      <th>word_freq_address</th>\n",
       "      <th>word_freq_all</th>\n",
       "      <th>word_freq_3d</th>\n",
       "      <th>word_freq_our</th>\n",
       "      <th>word_freq_over</th>\n",
       "      <th>word_freq_remove</th>\n",
       "      <th>word_freq_internet</th>\n",
       "      <th>word_freq_order</th>\n",
       "      <th>word_freq_mail</th>\n",
       "      <th>...</th>\n",
       "      <th>char_freq_;</th>\n",
       "      <th>char_freq_(</th>\n",
       "      <th>char_freq_[</th>\n",
       "      <th>char_freq_!</th>\n",
       "      <th>char_freq_$</th>\n",
       "      <th>char_freq_#</th>\n",
       "      <th>capital_run_length_average</th>\n",
       "      <th>capital_run_length_longest</th>\n",
       "      <th>capital_run_length_total</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2355</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.558</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.000</td>\n",
       "      <td>7</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1712</th>\n",
       "      <td>0.09</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.39</td>\n",
       "      <td>...</td>\n",
       "      <td>0.765</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.828</td>\n",
       "      <td>1.308</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.047</td>\n",
       "      <td>54</td>\n",
       "      <td>768</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>644</th>\n",
       "      <td>0.89</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.78</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.344</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.250</td>\n",
       "      <td>16</td>\n",
       "      <td>84</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2557</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.000</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1556</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.215</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.215</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.937</td>\n",
       "      <td>18</td>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      word_freq_make  word_freq_address  word_freq_all  word_freq_3d  \\\n",
       "2355            0.00               0.00           0.00           0.0   \n",
       "1712            0.09               0.49           0.59           0.0   \n",
       "644             0.89               0.00           0.89           0.0   \n",
       "2557            0.00               0.00           0.00           0.0   \n",
       "1556            0.00               0.00           0.00           0.0   \n",
       "\n",
       "      word_freq_our  word_freq_over  word_freq_remove  word_freq_internet  \\\n",
       "2355           0.00            0.00              0.00                 0.0   \n",
       "1712           0.39            0.19              0.00                 0.0   \n",
       "644            0.00            0.00              1.78                 0.0   \n",
       "2557           0.00            0.00              0.00                 0.0   \n",
       "1556           0.00            0.00              0.00                 0.0   \n",
       "\n",
       "      word_freq_order  word_freq_mail  ...  char_freq_;  char_freq_(  \\\n",
       "2355             0.00            0.00  ...        0.000        0.000   \n",
       "1712             0.09            0.39  ...        0.765        0.037   \n",
       "644              0.00            0.00  ...        0.000        0.000   \n",
       "2557             0.00            0.00  ...        0.000        0.000   \n",
       "1556             0.00            0.00  ...        0.000        0.215   \n",
       "\n",
       "      char_freq_[  char_freq_!  char_freq_$  char_freq_#  \\\n",
       "2355          0.0        0.558        0.000          0.0   \n",
       "1712          0.0        5.828        1.308          0.0   \n",
       "644           0.0        1.344        0.000          0.0   \n",
       "2557          0.0        0.000        0.000          0.0   \n",
       "1556          0.0        0.000        0.215          0.0   \n",
       "\n",
       "      capital_run_length_average  capital_run_length_longest  \\\n",
       "2355                       2.000                           7   \n",
       "1712                       6.047                          54   \n",
       "644                        5.250                          16   \n",
       "2557                       2.000                           4   \n",
       "1556                       3.937                          18   \n",
       "\n",
       "      capital_run_length_total  target  \n",
       "2355                        28       0  \n",
       "1712                       768       1  \n",
       "644                         84       1  \n",
       "2557                         6       0  \n",
       "1556                        63       1  \n",
       "\n",
       "[5 rows x 58 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "919441ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "word_freq_make                0\n",
       "word_freq_address             0\n",
       "word_freq_all                 0\n",
       "word_freq_3d                  0\n",
       "word_freq_our                 0\n",
       "word_freq_over                0\n",
       "word_freq_remove              0\n",
       "word_freq_internet            0\n",
       "word_freq_order               0\n",
       "word_freq_mail                0\n",
       "word_freq_receive             0\n",
       "word_freq_will                0\n",
       "word_freq_people              0\n",
       "word_freq_report              0\n",
       "word_freq_addresses           0\n",
       "word_freq_free                0\n",
       "word_freq_business            0\n",
       "word_freq_email               0\n",
       "word_freq_you                 0\n",
       "word_freq_credit              0\n",
       "word_freq_your                0\n",
       "word_freq_font                0\n",
       "word_freq_000                 0\n",
       "word_freq_money               0\n",
       "word_freq_hp                  0\n",
       "word_freq_hpl                 0\n",
       "word_freq_george              0\n",
       "word_freq_650                 0\n",
       "word_freq_lab                 0\n",
       "word_freq_labs                0\n",
       "word_freq_telnet              0\n",
       "word_freq_857                 0\n",
       "word_freq_data                0\n",
       "word_freq_415                 0\n",
       "word_freq_85                  0\n",
       "word_freq_technology          0\n",
       "word_freq_1999                0\n",
       "word_freq_parts               0\n",
       "word_freq_pm                  0\n",
       "word_freq_direct              0\n",
       "word_freq_cs                  0\n",
       "word_freq_meeting             0\n",
       "word_freq_original            0\n",
       "word_freq_project             0\n",
       "word_freq_re                  0\n",
       "word_freq_edu                 0\n",
       "word_freq_table               0\n",
       "word_freq_conference          0\n",
       "char_freq_;                   0\n",
       "char_freq_(                   0\n",
       "char_freq_[                   0\n",
       "char_freq_!                   0\n",
       "char_freq_$                   0\n",
       "char_freq_#                   0\n",
       "capital_run_length_average    0\n",
       "capital_run_length_longest    0\n",
       "capital_run_length_total      0\n",
       "target                        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "aed45341",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4601, 57), (4601,))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = spam_df.drop('target', axis=1)\n",
    "y = spam_df['target']\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "86df3066",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3220, 57), (1381, 57), (3220,), (1381,))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X ,y,\n",
    "                                                    test_size=0.3,\n",
    "                                                    random_state=123)\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c694c7e",
   "metadata": {},
   "source": [
    "#### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a69e299d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb = BernoulliNB()\n",
    "mnb = MultinomialNB()\n",
    "gnb = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d607f71a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BernoulliNB()"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bnb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "fe2e57be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5c6b538b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB()"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gnb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f1696816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Val score for Bernoulli :  [0.87267081 0.90993789 0.91925466 0.89130435 0.88198758 0.88198758\n",
      " 0.86024845 0.89130435 0.88819876 0.87267081]\n",
      "Mean cv score for Bernoulli 0.8869565217391303\n"
     ]
    }
   ],
   "source": [
    "bnb_cv = cross_val_score(bnb, X_train, y_train, cv=10)\n",
    "print(\"Cross Val score for Bernoulli : \",bnb_cv)\n",
    "print(\"Mean cv score for Bernoulli\", bnb_cv.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c24e1adc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Val score for Multinomial :  [0.80434783 0.79503106 0.78571429 0.75776398 0.80434783 0.78571429\n",
      " 0.80745342 0.77018634 0.77950311 0.80124224]\n",
      "Mean cv score for Multinomial 0.7891304347826087\n"
     ]
    }
   ],
   "source": [
    "mnb_cv = cross_val_score(mnb, X_train, y_train, cv=10)\n",
    "print(\"Cross Val score for Multinomial : \",mnb_cv)\n",
    "print(\"Mean cv score for Multinomial\", mnb_cv.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2ec64ab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Val score for Gaussian:  [0.85714286 0.81987578 0.82298137 0.81987578 0.81677019 0.80745342\n",
      " 0.83229814 0.82608696 0.80124224 0.81677019]\n",
      "Mean cv score for Gaussian 0.8220496894409937\n"
     ]
    }
   ],
   "source": [
    "gnb_cv = cross_val_score(gnb, X_train, y_train, cv=10)\n",
    "print(\"Cross Val score for Gaussian: \",gnb_cv)\n",
    "print(\"Mean cv score for Gaussian\", gnb_cv.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5bfb82",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3f1f5930",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_results(y_true, y_pred):\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    \n",
    "    print(\"Accuracy :\", accuracy)\n",
    "    print(\"Precision : \", precision)\n",
    "    print(\"Recall : \", recall)\n",
    "    print(\"F1 score : \", f1)\n",
    "    \n",
    "    return {'accuracy': accuracy,'precision':precision,'recall':recall,'f1':f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a2c36de3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.8834178131788559\n",
      "Precision :  0.898\n",
      "Recall :  0.8032200357781754\n",
      "F1 score :  0.8479697828139755\n"
     ]
    }
   ],
   "source": [
    "y_pred_bnb = bnb.predict(X_test)\n",
    "bnb_score = cal_results(y_test,y_pred_bnb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ccfa9117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.775524981897176\n",
      "Precision :  0.7398843930635838\n",
      "Recall :  0.6869409660107334\n",
      "F1 score :  0.712430426716141\n"
     ]
    }
   ],
   "source": [
    "y_pred_mnb = mnb.predict(X_test)\n",
    "mnb_score = cal_results(y_test,y_pred_mnb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "2f8a71a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.8167994207096307\n",
      "Precision :  0.7007874015748031\n",
      "Recall :  0.9552772808586762\n",
      "F1 score :  0.8084784254352764\n"
     ]
    }
   ],
   "source": [
    "y_pred_gnb = gnb.predict(X_test)\n",
    "gnb_score = cal_results(y_test,y_pred_gnb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19d1186",
   "metadata": {},
   "source": [
    "#### Discussion\n",
    "\n",
    "Based on the results obtained, Bernoulli Naive Bayes performed the best with an accuracy of 0.8834, followed by Gaussian Naive Bayes with an accuracy of 0.8168 and Multinomial Naive Bayes with an accuracy of 0.7755.\n",
    "\n",
    "One possible reason for the superior performance of Bernoulli Naive Bayes could be that it is specifically designed for binary features, which is the case for the Spambase dataset. In contrast, Multinomial Naive Bayes and Gaussian Naive Bayes are better suited for discrete and continuous features, respectively, which may explain their lower performance on this particular dataset.\n",
    "\n",
    "It is also worth noting that the precision and recall values for each classifier vary, with Bernoulli Naive Bayes having the highest precision but a lower recall compared to Gaussian Naive Bayes, which has the highest recall but a lower precision. This trade-off between precision and recall is a common issue in classification problems, and the choice of the appropriate metric depends on the specific application.\n",
    "\n",
    "Limitations of Naive Bayes observed include the assumption of independence between features, which may not hold true in some real-world scenarios, and the sensitivity to imbalanced class distributions, which can lead to biased predictions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491edff0",
   "metadata": {},
   "source": [
    "#### Conclusion\n",
    "\n",
    "In conclusion, our evaluation of three variants of Naive Bayes classifiers on the Spambase dataset showed that Bernoulli Naive Bayes outperformed Multinomial and Gaussian Naive Bayes in terms of accuracy. However, the choice of the appropriate variant depends on the specific characteristics of the dataset and the desired trade-off between precision and recall. Future work could focus on exploring the use of more advanced techniques, such as ensemble methods or deep learning, to further improve the performance of spam classification models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91685cc6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
