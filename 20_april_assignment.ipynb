{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b856bd5",
   "metadata": {},
   "source": [
    "### Q1. What is the KNN algorithm?\n",
    "\n",
    "The K-nearest neighbors (KNN) algorithm is a supervised machine learning algorithm used for both classification and regression tasks. It works by finding the K closest training examples in the feature space to the new data point and predicting the output based on the most common class (in classification) or the average of the K nearest neighbors (in regression)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d607a416",
   "metadata": {},
   "source": [
    "### Q2. How do you choose the value of K in KNN?\n",
    "\n",
    " The choice of K depends on the problem and the data. A smaller value of K means that the decision boundary is more complex, but it may be more susceptible to noise. A larger value of K means that the decision boundary is smoother, but it may be less sensitive to local features. One common method to choose K is to use cross-validation to find the value of K that results in the highest accuracy or lowest error rate on the validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328a040d",
   "metadata": {},
   "source": [
    "### Q3. What is the difference between KNN classifier and KNN regressor?\n",
    "\n",
    "The KNN classifier predicts the class label of a new data point based on the K nearest neighbors, while the KNN regressor predicts a continuous output variable based on the average of the K nearest neighbors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957cbaf0",
   "metadata": {},
   "source": [
    "### Q4. How do you measure the performance of KNN?\n",
    "\n",
    "The performance of KNN can be measured using metrics such as accuracy, precision, recall, F1-score, mean squared error, and R-squared. Cross-validation can also be used to estimate the performance on unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955dd803",
   "metadata": {},
   "source": [
    "### Q5. What is the curse of dimensionality in KNN?\n",
    "\n",
    "The curse of dimensionality in KNN refers to the phenomenon where the performance of the algorithm deteriorates as the number of features or dimensions increases. This is because the number of training examples required to cover the feature space increases exponentially with the number of dimensions, making it more difficult to find nearest neighbors that are relevant to the new data point. One way to address this is to use feature selection or dimensionality reduction techniques to reduce the number of features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f601de",
   "metadata": {},
   "source": [
    "### Q6. How do you handle missing values in KNN?\n",
    "\n",
    "One common approach to handling missing values in KNN is to impute the missing values with the average or median value of the corresponding feature in the training set. Another approach is to use a variant of KNN that is specifically designed to handle missing values, such as the K-nearest imputation algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f98dbe8",
   "metadata": {},
   "source": [
    "### Q7. Compare and contrast the performance of the KNN classifier and regressor. Which one is better for which type of problem?\n",
    "\n",
    "The performance of KNN classifier and regressor depends on the problem and the data. In general, the KNN classifier is better suited for problems where the output variable is discrete and the decision boundary is complex, while the KNN regressor is better suited for problems where the output variable is continuous and the relationship between the features and the output is smooth."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8797b151",
   "metadata": {},
   "source": [
    "### Q8. What are the strengths and weaknesses of the KNN algorithm for classification and regression tasks, and how can these be addressed?\n",
    "\n",
    "The strengths of KNN include its simplicity, interpretability, and ability to handle nonlinear relationships. Its weaknesses include its sensitivity to noise, the need to choose an appropriate value of K, the curse of dimensionality, and the lack of scalability. These weaknesses can be addressed by using techniques such as feature selection, distance weighting, and approximations of the KNN algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e4336d",
   "metadata": {},
   "source": [
    "### Q9. What is the difference between Euclidean distance and Manhattan distance in KNN?\n",
    "\n",
    "Euclidean distance is the straight-line distance between two points in a Euclidean space, while Manhattan distance is the sum of the absolute differences between the coordinates of two points in a Euclidean space. In KNN, Euclidean distance is commonly used when the features are continuous and have a natural geometric interpretation, while Manhattan distance is commonly used when the features are discrete or represent counts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018293ac",
   "metadata": {},
   "source": [
    "### Q10. What is the role of feature scaling in KNN?\n",
    "\n",
    "Feature scaling is important in KNN because the distance metric used to compute the similarity between data points is sensitive to the scale of the features. Features with larger scales may dominate the distance metric, leading to biased results. Therefore, it is important to normalize or standardize the features to have zero mean and unit variance before using KNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51d5ddb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
